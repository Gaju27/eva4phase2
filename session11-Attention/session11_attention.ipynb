{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session11-attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_sHCKODLN3n",
        "outputId": "4851dba8-133a-4e8c-d205-7c2214e36551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch numpy matplotlib sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.4.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kCWZTbjLZ63",
        "outputId": "5577bd17-3bac-4625-9cc2-aeebbf5f3742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "# we will use CUDA if it is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
        "print(\"CUDA:\", USE_CUDA)\n",
        "print(DEVICE)\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA: True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TFL5kfkLfIz"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
        "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
        "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
        "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask, src_lengths):\n",
        "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
        "    \n",
        "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
        "               decoder_hidden=None):\n",
        "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
        "                            src_mask, trg_mask, hidden=decoder_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiTJuwE2LjTD"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m670zifLmaL"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
        "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Applies a bidirectional GRU to sequence of embeddings x.\n",
        "        The input mini-batch x needs to be sorted by length.\n",
        "        x should have dimensions [batch, time, dim].\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        output, final = self.rnn(packed)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # we need to manually concatenate the final states for both directions\n",
        "        fwd_final = final[0:final.size(0):2]\n",
        "        bwd_final = final[1:final.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        return output, final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqs3SUZ2LqtM"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
        "                 bridge=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "                 \n",
        "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "                 \n",
        "        # to initialize from the final encoder state\n",
        "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
        "                                          hidden_size, bias=False)\n",
        "        \n",
        "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
        "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, proj_key=proj_key,\n",
        "            value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "        # update rnn hidden state\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        \n",
        "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "        pre_output = self.dropout_layer(pre_output)\n",
        "        pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "        return output, hidden, pre_output\n",
        "    \n",
        "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
        "                src_mask, trg_mask, hidden=None, max_len=None):\n",
        "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
        "                                         \n",
        "        # the maximum number of steps to unroll the RNN\n",
        "        if max_len is None:\n",
        "            max_len = trg_mask.size(-1)\n",
        "\n",
        "        # initialize decoder hidden state\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(encoder_final)\n",
        "        \n",
        "        # pre-compute projected encoder hidden states\n",
        "        # (the \"keys\" for the attention mechanism)\n",
        "        # this is only done for efficiency\n",
        "        proj_key = self.attention.key_layer(encoder_hidden)\n",
        "        \n",
        "        # here we store all intermediate hidden states and pre-output vectors\n",
        "        decoder_states = []\n",
        "        pre_output_vectors = []\n",
        "        \n",
        "        # unroll the decoder RNN for max_len steps\n",
        "        for i in range(max_len):\n",
        "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
        "            output, hidden, pre_output = self.forward_step(\n",
        "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
        "            decoder_states.append(output)\n",
        "            pre_output_vectors.append(pre_output)\n",
        "\n",
        "        decoder_states = torch.cat(decoder_states, dim=1)\n",
        "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
        "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
        "\n",
        "    def init_hidden(self, encoder_final):\n",
        "        \"\"\"Returns the initial decoder state,\n",
        "        conditioned on the final encoder state.\"\"\"\n",
        "\n",
        "        if encoder_final is None:\n",
        "            return None  # start with zeros\n",
        "\n",
        "        return torch.tanh(self.bridge(encoder_final))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTTNF9-nLtkk"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
        "        key_size = 2 * hidden_size if key_size is None else key_size\n",
        "        query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "        \n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "        assert mask is not None, \"mask is required\"\n",
        "\n",
        "        # We first project the query (the decoder state).\n",
        "        # The projected keys (the encoder states) were already pre-computated.\n",
        "        query = self.query_layer(query)\n",
        "        \n",
        "        # Calculate scores.\n",
        "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        \n",
        "        # Mask out invalid positions.\n",
        "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "        \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=-1)\n",
        "        self.alphas = alphas        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        context = torch.bmm(alphas, value)\n",
        "        \n",
        "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "        return context, alphas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68isSibLwoj"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "\n",
        "    attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
        "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
        "        nn.Embedding(src_vocab, emb_size),\n",
        "        nn.Embedding(tgt_vocab, emb_size),\n",
        "        Generator(hidden_size, tgt_vocab))\n",
        "\n",
        "    return model.cuda() if USE_CUDA else model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt7AkwmNLzOL"
      },
      "source": [
        "class Batch:\n",
        "    \"\"\"Object for holding a batch of data with mask during training.\n",
        "    Input is a batch from a torch text iterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, trg, pad_index=0):\n",
        "        \n",
        "        src, src_lengths = src\n",
        "        \n",
        "        self.src = src\n",
        "        self.src_lengths = src_lengths\n",
        "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
        "        self.nseqs = src.size(0)\n",
        "        \n",
        "        self.trg = None\n",
        "        self.trg_y = None\n",
        "        self.trg_mask = None\n",
        "        self.trg_lengths = None\n",
        "        self.ntokens = None\n",
        "\n",
        "        if trg is not None:\n",
        "            trg, trg_lengths = trg\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_lengths = trg_lengths\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = (self.trg_y != pad_index)\n",
        "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            self.src = self.src.cuda()\n",
        "            self.src_mask = self.src_mask.cuda()\n",
        "\n",
        "            if trg is not None:\n",
        "                self.trg = self.trg.cuda()\n",
        "                self.trg_y = self.trg_y.cuda()\n",
        "                self.trg_mask = self.trg_mask.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mamPT3VLL2j0"
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
        "    \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    print_tokens = 0\n",
        "\n",
        "    for i, batch in enumerate(data_iter, 1):\n",
        "        \n",
        "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
        "                                           batch.src_mask, batch.trg_mask,\n",
        "                                           batch.src_lengths, batch.trg_lengths)\n",
        "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        print_tokens += batch.ntokens\n",
        "        \n",
        "        if model.training and i % print_every == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
        "            start = time.time()\n",
        "            print_tokens = 0\n",
        "\n",
        "    return math.exp(total_loss / float(total_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpj5VnOOL4mb"
      },
      "source": [
        "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
        "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
        "    for i in range(num_batches):\n",
        "        data = torch.from_numpy(\n",
        "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
        "        data[:, 0] = sos_index\n",
        "        data = data.cuda() if USE_CUDA else data\n",
        "        src = data[:, 1:]\n",
        "        trg = data\n",
        "        src_lengths = [length-1] * batch_size\n",
        "        trg_lengths = [length] * batch_size\n",
        "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obuU-Il1L7P7"
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                              y.contiguous().view(-1))\n",
        "        loss = loss / norm\n",
        "\n",
        "        if self.opt is not None:\n",
        "            loss.backward()          \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        return loss.data.item() * norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_utEZiS7L9y8"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
        "    \"\"\"Greedily decode a sentence.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
        "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
        "        trg_mask = torch.ones_like(prev_y)\n",
        "\n",
        "    output = []\n",
        "    attention_scores = []\n",
        "    hidden = None\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out, hidden, pre_output = model.decode(\n",
        "              encoder_hidden, encoder_final, src_mask,\n",
        "              prev_y, trg_mask, hidden)\n",
        "\n",
        "            # we predict from the pre-output layer, which is\n",
        "            # a combination of Decoder state, prev emb, and context\n",
        "            prob = model.generator(pre_output[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data.item()\n",
        "        output.append(next_word)\n",
        "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
        "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
        "    \n",
        "    output = np.array(output)\n",
        "        \n",
        "    # cut off everything starting from </s> \n",
        "    # (only when eos_index provided)\n",
        "    if eos_index is not None:\n",
        "        first_eos = np.where(output==eos_index)[0]\n",
        "        if len(first_eos) > 0:\n",
        "            output = output[:first_eos[0]]      \n",
        "    \n",
        "    return output, np.concatenate(attention_scores, axis=1)\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab=None):\n",
        "    if vocab is not None:\n",
        "        x = [vocab.itos[i] for i in x]\n",
        "\n",
        "    return [str(t) for t in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-IOH_KqMANc"
      },
      "source": [
        "def print_examples(example_iter, model, n=2, max_len=100, \n",
        "                   sos_index=1, \n",
        "                   src_eos_index=None, \n",
        "                   trg_eos_index=None, \n",
        "                   src_vocab=None, trg_vocab=None):\n",
        "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    print()\n",
        "    \n",
        "    if src_vocab is not None and trg_vocab is not None:\n",
        "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
        "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
        "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
        "    else:\n",
        "        src_eos_index = None\n",
        "        trg_sos_index = 1\n",
        "        trg_eos_index = None\n",
        "        \n",
        "    for i, batch in enumerate(example_iter):\n",
        "      \n",
        "        src = batch.src.cpu().numpy()[0, :]\n",
        "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
        "\n",
        "        # remove </s> (if it is there)\n",
        "        src = src[:-1] if src[-1] == src_eos_index else src\n",
        "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
        "      \n",
        "        result, _ = greedy_decode(\n",
        "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
        "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
        "        print(\"Example #%d\" % (i+1))\n",
        "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
        "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
        "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
        "        print()\n",
        "        \n",
        "        count += 1\n",
        "        if count == n:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpA0D6nOMCKj"
      },
      "source": [
        "def train_copy_task():\n",
        "    \"\"\"Train the simple copy task.\"\"\"\n",
        "    num_words = 11\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=0)\n",
        "    model = make_model(num_words, num_words, emb_size=32, hidden_size=64)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
        "    eval_data = list(data_gen(num_words=num_words, batch_size=1, num_batches=100))\n",
        " \n",
        "    dev_perplexities = []\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        \n",
        "        print(\"Epoch %d\" % epoch)\n",
        "\n",
        "        # train\n",
        "        model.train()\n",
        "        data = data_gen(num_words=num_words, batch_size=32, num_batches=100)\n",
        "        run_epoch(data, model,\n",
        "                  SimpleLossCompute(model.generator, criterion, optim))\n",
        "\n",
        "        # evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad(): \n",
        "            perplexity = run_epoch(eval_data, model,\n",
        "                                   SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Evaluation perplexity: %f\" % perplexity)\n",
        "            dev_perplexities.append(perplexity)\n",
        "            print_examples(eval_data, model, n=2, max_len=9)\n",
        "        \n",
        "    return dev_perplexities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56bRN4UMEDD",
        "outputId": "4d672c4c-0631-41b1-9e77-bb22af373cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the copy task\n",
        "dev_perplexities = train_copy_task()\n",
        "\n",
        "def plot_perplexity(perplexities):\n",
        "    \"\"\"plot perplexities\"\"\"\n",
        "    plt.title(\"Perplexity per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.plot(perplexities)\n",
        "    \n",
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch Step: 50 Loss: 19.720032 Tokens per Sec: 11532.560066\n",
            "Epoch Step: 100 Loss: 17.850552 Tokens per Sec: 14986.094622\n",
            "Evaluation perplexity: 7.165516\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  5 8 7 5 8 7 5 8 7\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 8 8 8 8 8 8 8\n",
            "\n",
            "Epoch 1\n",
            "Epoch Step: 50 Loss: 15.429652 Tokens per Sec: 14914.005189\n",
            "Epoch Step: 100 Loss: 11.758204 Tokens per Sec: 15256.591523\n",
            "Evaluation perplexity: 3.761093\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 5 3 8 7 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 8 2 5 8 3 2\n",
            "\n",
            "Epoch 2\n",
            "Epoch Step: 50 Loss: 9.917423 Tokens per Sec: 14373.540757\n",
            "Epoch Step: 100 Loss: 8.949948 Tokens per Sec: 15009.825701\n",
            "Evaluation perplexity: 2.573576\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 5 7 8 10\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 8 5 2 6 8 2\n",
            "\n",
            "Epoch 3\n",
            "Epoch Step: 50 Loss: 7.275528 Tokens per Sec: 14733.143731\n",
            "Epoch Step: 100 Loss: 6.582399 Tokens per Sec: 14739.284706\n",
            "Evaluation perplexity: 2.072119\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 5 8 7 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 2 5 8 2 6\n",
            "\n",
            "Epoch 4\n",
            "Epoch Step: 50 Loss: 5.942049 Tokens per Sec: 14918.212017\n",
            "Epoch Step: 100 Loss: 4.906624 Tokens per Sec: 15137.868898\n",
            "Evaluation perplexity: 1.765160\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 5 10 8 7\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 5\n",
            "Epoch Step: 50 Loss: 5.266298 Tokens per Sec: 15158.875505\n",
            "Epoch Step: 100 Loss: 4.140487 Tokens per Sec: 15267.968802\n",
            "Evaluation perplexity: 1.565404\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 3 10 5 7 8\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 6\n",
            "Epoch Step: 50 Loss: 3.958579 Tokens per Sec: 15095.101725\n",
            "Epoch Step: 100 Loss: 3.681249 Tokens per Sec: 14944.168449\n",
            "Evaluation perplexity: 1.455613\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 5 8 7\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 8 6\n",
            "\n",
            "Epoch 7\n",
            "Epoch Step: 50 Loss: 2.987803 Tokens per Sec: 15171.738425\n",
            "Epoch Step: 100 Loss: 2.790762 Tokens per Sec: 15088.785135\n",
            "Evaluation perplexity: 1.366846\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 8\n",
            "Epoch Step: 50 Loss: 2.148364 Tokens per Sec: 15117.843683\n",
            "Epoch Step: 100 Loss: 2.303624 Tokens per Sec: 15160.458393\n",
            "Evaluation perplexity: 1.275108\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n",
            "Epoch 9\n",
            "Epoch Step: 50 Loss: 2.088928 Tokens per Sec: 14865.771031\n",
            "Epoch Step: 100 Loss: 2.050288 Tokens per Sec: 15256.344882\n",
            "Evaluation perplexity: 1.195517\n",
            "\n",
            "Example #1\n",
            "Src :  4 8 5 7 10 3 7 8 5\n",
            "Trg :  4 8 5 7 10 3 7 8 5\n",
            "Pred:  4 8 5 7 10 3 7 8 5\n",
            "\n",
            "Example #2\n",
            "Src :  8 8 3 6 5 2 8 6 2\n",
            "Trg :  8 8 3 6 5 2 8 6 2\n",
            "Pred:  8 8 3 6 5 2 8 6 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MVkipyuMGAD",
        "outputId": "a021881e-5447-4f67-890b-4595dc160219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+git://github.com/pytorch/text spacy \n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/pytorch/text\n",
            "  Cloning git://github.com/pytorch/text to /tmp/pip-req-build-ksyoo0fh\n",
            "  Running command git clone -q git://github.com/pytorch/text /tmp/pip-req-build-ksyoo0fh\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.9.0a0+97e6d1d) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.9.0a0+97e6d1d) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.9.0a0+97e6d1d) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchtext: filename=torchtext-0.9.0a0+97e6d1d-cp36-cp36m-linux_x86_64.whl size=6980261 sha256=05c58814d3feb9529db73eec2cd1e00e3a2e25552f2c816b6b4ac54fc7faee4f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-29hwzdze/wheels/39/42/ff/82f5ccbb0f30b25e14610376f5d0c67913fc05017dab59f8eb\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.9.0a0+97e6d1d\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.1)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=69e18735d123f519543675f0dcc1f9eafbc9b25dd4b85d8c6eaf109ac7b7b5e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_rqaz2v4/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02LqW501NI7j",
        "outputId": "debfe928-a84c-43f3-f189-6f27c496a978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets\n",
        "\n",
        "if True:\n",
        "    import spacy\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    UNK_TOKEN = \"<unk>\"\n",
        "    PAD_TOKEN = \"<pad>\"    \n",
        "    SOS_TOKEN = \"<s>\"\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    LOWER = True\n",
        "    \n",
        "    # we include lengths to provide to the RNNs\n",
        "    SRC = data.Field(tokenize=tokenize_de, \n",
        "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
        "    TRG = data.Field(tokenize=tokenize_en, \n",
        "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
        "\n",
        "    MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
        "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
        "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
        "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
        "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
        "    \n",
        "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:05<00:00, 4.15MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVU2f4I5MMcT",
        "outputId": "f526b9b4-9d8f-4411-d82f-8bf4c13c4021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
        "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
        "\n",
        "    print(\"Data set sizes (number of sentence pairs):\")\n",
        "    print('train', len(train_data))\n",
        "    print('valid', len(valid_data))\n",
        "    print('test', len(test_data), \"\\n\")\n",
        "\n",
        "    print(\"First training example:\")\n",
        "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
        "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
        "\n",
        "    print(\"Most common words (src):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "    print(\"Most common words (trg):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "\n",
        "    print(\"First 10 words (src):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
        "    print(\"First 10 words (trg):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
        "\n",
        "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
        "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
        "    \n",
        "    \n",
        "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set sizes (number of sentence pairs):\n",
            "train 143115\n",
            "valid 690\n",
            "test 963 \n",
            "\n",
            "First training example:\n",
            "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
            "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
            "\n",
            "Most common words (src):\n",
            "         .     138329\n",
            "         ,     105944\n",
            "       und      41843\n",
            "       die      40808\n",
            "       das      33324\n",
            "       sie      33034\n",
            "       ich      31150\n",
            "       ist      31037\n",
            "        es      27449\n",
            "       wir      25817 \n",
            "\n",
            "Most common words (trg):\n",
            "         .     137259\n",
            "         ,      91615\n",
            "       the      73343\n",
            "       and      50276\n",
            "        to      42799\n",
            "         a      39572\n",
            "        of      39496\n",
            "         i      33521\n",
            "        it      32920\n",
            "      that      32640 \n",
            "\n",
            "First 10 words (src):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 </s>\n",
            "03 .\n",
            "04 ,\n",
            "05 und\n",
            "06 die\n",
            "07 das\n",
            "08 sie\n",
            "09 ich \n",
            "\n",
            "First 10 words (trg):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 <s>\n",
            "03 </s>\n",
            "04 .\n",
            "05 ,\n",
            "06 the\n",
            "07 and\n",
            "08 to\n",
            "09 a \n",
            "\n",
            "Number of German words (types): 15765\n",
            "Number of English words (types): 13002 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVXeEy0HNVYc",
        "outputId": "6c501711-06f6-46ca-af2c-99cb14bd9717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_iter = data.BucketIterator(train_data, batch_size=64, train=True, \n",
        "                                 sort_within_batch=True, \n",
        "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
        "                                 device=DEVICE)\n",
        "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
        "                           device=DEVICE)\n",
        "\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
        "    return Batch(batch.src, batch.trg, pad_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1xvZri7NcR9"
      },
      "source": [
        "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
        "    \"\"\"Train a model on IWSLT\"\"\"\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    # optionally add label smoothing; see the Annotated Transformer\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    dev_perplexities = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        print(\"Epoch\", epoch)\n",
        "        model.train()\n",
        "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
        "                                     model,\n",
        "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
        "                                     print_every=print_every)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
        "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
        "\n",
        "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
        "                                       model, \n",
        "                                       SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
        "            dev_perplexities.append(dev_perplexity)\n",
        "        \n",
        "    return dev_perplexities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "990yI2T-Nlrh",
        "outputId": "ed47ef62-3e84-407f-df48-80163432a649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)\n",
        "dev_perplexities = train(model, print_every=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 100 Loss: 116.590652 Tokens per Sec: 22602.028367\n",
            "Epoch Step: 200 Loss: 45.832985 Tokens per Sec: 23930.318617\n",
            "Epoch Step: 300 Loss: 50.010319 Tokens per Sec: 23764.277514\n",
            "Epoch Step: 400 Loss: 84.559731 Tokens per Sec: 23311.010039\n",
            "Epoch Step: 500 Loss: 84.707306 Tokens per Sec: 23622.070958\n",
            "Epoch Step: 600 Loss: 52.742657 Tokens per Sec: 23830.411969\n",
            "Epoch Step: 700 Loss: 84.035965 Tokens per Sec: 23597.579415\n",
            "Epoch Step: 800 Loss: 60.506886 Tokens per Sec: 24105.930936\n",
            "Epoch Step: 900 Loss: 50.634277 Tokens per Sec: 23488.550361\n",
            "Epoch Step: 1000 Loss: 58.911953 Tokens per Sec: 23713.617462\n",
            "Epoch Step: 1100 Loss: 18.165363 Tokens per Sec: 23629.999912\n",
            "Epoch Step: 1200 Loss: 111.021767 Tokens per Sec: 23795.966721\n",
            "Epoch Step: 1300 Loss: 72.016121 Tokens per Sec: 23716.446488\n",
            "Epoch Step: 1400 Loss: 61.338413 Tokens per Sec: 23563.748447\n",
            "Epoch Step: 1500 Loss: 33.586964 Tokens per Sec: 23492.431068\n",
            "Epoch Step: 1600 Loss: 39.442581 Tokens per Sec: 23942.135215\n",
            "Epoch Step: 1700 Loss: 54.487888 Tokens per Sec: 23537.158181\n",
            "Epoch Step: 1800 Loss: 51.632637 Tokens per Sec: 23241.288811\n",
            "Epoch Step: 1900 Loss: 50.578484 Tokens per Sec: 23148.401401\n",
            "Epoch Step: 2000 Loss: 27.128759 Tokens per Sec: 22940.675763\n",
            "Epoch Step: 2100 Loss: 79.417458 Tokens per Sec: 23224.036934\n",
            "Epoch Step: 2200 Loss: 98.671440 Tokens per Sec: 23542.399654\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was born years old , i was one of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on the same way , the <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he was very much from what was very much , there was the most important to be the <unk> .\n",
            "\n",
            "Validation perplexity: 30.884657\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 85.270721 Tokens per Sec: 22434.520600\n",
            "Epoch Step: 200 Loss: 37.326672 Tokens per Sec: 23569.445515\n",
            "Epoch Step: 300 Loss: 61.608555 Tokens per Sec: 23662.436061\n",
            "Epoch Step: 400 Loss: 17.097485 Tokens per Sec: 23661.692314\n",
            "Epoch Step: 500 Loss: 81.460609 Tokens per Sec: 23633.698925\n",
            "Epoch Step: 600 Loss: 50.053955 Tokens per Sec: 23460.312327\n",
            "Epoch Step: 700 Loss: 7.388033 Tokens per Sec: 23810.337542\n",
            "Epoch Step: 800 Loss: 14.795426 Tokens per Sec: 23576.687313\n",
            "Epoch Step: 900 Loss: 68.213051 Tokens per Sec: 23272.407415\n",
            "Epoch Step: 1000 Loss: 8.415938 Tokens per Sec: 23383.027002\n",
            "Epoch Step: 1100 Loss: 74.379822 Tokens per Sec: 23790.960014\n",
            "Epoch Step: 1200 Loss: 57.094513 Tokens per Sec: 23518.563754\n",
            "Epoch Step: 1300 Loss: 57.908604 Tokens per Sec: 23538.537607\n",
            "Epoch Step: 1400 Loss: 77.475487 Tokens per Sec: 23280.265839\n",
            "Epoch Step: 1500 Loss: 35.239777 Tokens per Sec: 23411.681942\n",
            "Epoch Step: 1600 Loss: 53.447987 Tokens per Sec: 23400.825623\n",
            "Epoch Step: 1700 Loss: 71.780769 Tokens per Sec: 23435.302917\n",
            "Epoch Step: 1800 Loss: 70.113174 Tokens per Sec: 23673.669616\n",
            "Epoch Step: 1900 Loss: 83.944046 Tokens per Sec: 23436.492968\n",
            "Epoch Step: 2000 Loss: 21.762844 Tokens per Sec: 23708.527470\n",
            "Epoch Step: 2100 Loss: 15.367150 Tokens per Sec: 23483.869970\n",
            "Epoch Step: 2200 Loss: 45.550560 Tokens per Sec: 23614.936468\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy from what was the most interesting thing there was there for the news .\n",
            "\n",
            "Validation perplexity: 19.593108\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 57.302231 Tokens per Sec: 22582.589816\n",
            "Epoch Step: 200 Loss: 76.740517 Tokens per Sec: 23423.884650\n",
            "Epoch Step: 300 Loss: 33.435249 Tokens per Sec: 23741.547797\n",
            "Epoch Step: 400 Loss: 34.920647 Tokens per Sec: 23522.261362\n",
            "Epoch Step: 500 Loss: 7.784409 Tokens per Sec: 23562.849898\n",
            "Epoch Step: 600 Loss: 59.857437 Tokens per Sec: 23252.118983\n",
            "Epoch Step: 700 Loss: 15.718849 Tokens per Sec: 23288.578742\n",
            "Epoch Step: 800 Loss: 53.355755 Tokens per Sec: 23466.419560\n",
            "Epoch Step: 900 Loss: 48.049232 Tokens per Sec: 23372.590272\n",
            "Epoch Step: 1000 Loss: 30.563211 Tokens per Sec: 23436.403066\n",
            "Epoch Step: 1100 Loss: 9.935953 Tokens per Sec: 23453.913450\n",
            "Epoch Step: 1200 Loss: 7.333763 Tokens per Sec: 23380.002792\n",
            "Epoch Step: 1300 Loss: 62.559784 Tokens per Sec: 23436.390769\n",
            "Epoch Step: 1400 Loss: 27.693417 Tokens per Sec: 23410.159221\n",
            "Epoch Step: 1500 Loss: 19.952488 Tokens per Sec: 23338.967866\n",
            "Epoch Step: 1600 Loss: 23.574490 Tokens per Sec: 23602.131990\n",
            "Epoch Step: 1700 Loss: 53.227882 Tokens per Sec: 23574.501086\n",
            "Epoch Step: 1800 Loss: 53.506805 Tokens per Sec: 23151.188944\n",
            "Epoch Step: 1900 Loss: 33.429676 Tokens per Sec: 23465.454686\n",
            "Epoch Step: 2000 Loss: 71.049347 Tokens per Sec: 23496.187497\n",
            "Epoch Step: 2100 Loss: 14.235188 Tokens per Sec: 23326.482680\n",
            "Epoch Step: 2200 Loss: 65.569061 Tokens per Sec: 23409.095941\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> from the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , little radio <unk> , the radio of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , what was a lot of unusual , there was the news <unk> .\n",
            "\n",
            "Validation perplexity: 15.781252\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 30.251337 Tokens per Sec: 22422.008113\n",
            "Epoch Step: 200 Loss: 42.858330 Tokens per Sec: 23397.656590\n",
            "Epoch Step: 300 Loss: 62.533619 Tokens per Sec: 23503.437081\n",
            "Epoch Step: 400 Loss: 52.213989 Tokens per Sec: 23435.259235\n",
            "Epoch Step: 500 Loss: 25.810810 Tokens per Sec: 22945.676017\n",
            "Epoch Step: 600 Loss: 27.522142 Tokens per Sec: 22804.916859\n",
            "Epoch Step: 700 Loss: 41.135525 Tokens per Sec: 23201.599335\n",
            "Epoch Step: 800 Loss: 51.387089 Tokens per Sec: 23509.357500\n",
            "Epoch Step: 900 Loss: 54.179344 Tokens per Sec: 23263.386729\n",
            "Epoch Step: 1000 Loss: 33.724251 Tokens per Sec: 23170.933755\n",
            "Epoch Step: 1100 Loss: 19.541626 Tokens per Sec: 23243.226755\n",
            "Epoch Step: 1200 Loss: 43.181637 Tokens per Sec: 23353.801619\n",
            "Epoch Step: 1300 Loss: 31.310461 Tokens per Sec: 23114.109914\n",
            "Epoch Step: 1400 Loss: 29.191721 Tokens per Sec: 23375.854079\n",
            "Epoch Step: 1500 Loss: 47.209072 Tokens per Sec: 23443.305717\n",
            "Epoch Step: 1600 Loss: 22.189644 Tokens per Sec: 23489.600009\n",
            "Epoch Step: 1700 Loss: 32.579704 Tokens per Sec: 23581.101597\n",
            "Epoch Step: 1800 Loss: 53.701275 Tokens per Sec: 23364.626623\n",
            "Epoch Step: 1900 Loss: 26.568480 Tokens per Sec: 23363.338385\n",
            "Epoch Step: 2000 Loss: 42.140438 Tokens per Sec: 22996.063018\n",
            "Epoch Step: 2100 Loss: 31.466812 Tokens per Sec: 23159.905340\n",
            "Epoch Step: 2200 Loss: 53.086758 Tokens per Sec: 23029.081279\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was born by the morning of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , gray radio the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because it was the news of the most expensive .\n",
            "\n",
            "Validation perplexity: 13.930623\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 60.539989 Tokens per Sec: 21616.230402\n",
            "Epoch Step: 200 Loss: 11.706999 Tokens per Sec: 23305.801454\n",
            "Epoch Step: 300 Loss: 58.512520 Tokens per Sec: 23307.286780\n",
            "Epoch Step: 400 Loss: 10.329287 Tokens per Sec: 23224.141902\n",
            "Epoch Step: 500 Loss: 49.982677 Tokens per Sec: 23296.856386\n",
            "Epoch Step: 600 Loss: 45.934742 Tokens per Sec: 22928.205171\n",
            "Epoch Step: 700 Loss: 63.132038 Tokens per Sec: 23447.249254\n",
            "Epoch Step: 800 Loss: 52.182266 Tokens per Sec: 23443.069846\n",
            "Epoch Step: 900 Loss: 31.042080 Tokens per Sec: 23320.422155\n",
            "Epoch Step: 1000 Loss: 23.585728 Tokens per Sec: 23643.802950\n",
            "Epoch Step: 1100 Loss: 33.003193 Tokens per Sec: 23276.404455\n",
            "Epoch Step: 1200 Loss: 57.899170 Tokens per Sec: 23274.200573\n",
            "Epoch Step: 1300 Loss: 30.689161 Tokens per Sec: 23240.035700\n",
            "Epoch Step: 1400 Loss: 8.364587 Tokens per Sec: 23275.740823\n",
            "Epoch Step: 1500 Loss: 49.537033 Tokens per Sec: 23397.304509\n",
            "Epoch Step: 1600 Loss: 25.336517 Tokens per Sec: 23460.322165\n",
            "Epoch Step: 1700 Loss: 28.658348 Tokens per Sec: 23487.937483\n",
            "Epoch Step: 1800 Loss: 61.223885 Tokens per Sec: 23155.063505\n",
            "Epoch Step: 1900 Loss: 55.294415 Tokens per Sec: 23416.106862\n",
            "Epoch Step: 2000 Loss: 13.504469 Tokens per Sec: 23132.195922\n",
            "Epoch Step: 2100 Loss: 25.793707 Tokens per Sec: 23356.051100\n",
            "Epoch Step: 2200 Loss: 29.894430 Tokens per Sec: 23350.309626\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was born by the morning of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad stopped on his little , gray radio the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was quite unusual , which was the most exciting thing that was <unk> .\n",
            "\n",
            "Validation perplexity: 12.594349\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 28.825157 Tokens per Sec: 22361.065940\n",
            "Epoch Step: 200 Loss: 34.824757 Tokens per Sec: 23261.615931\n",
            "Epoch Step: 300 Loss: 44.328575 Tokens per Sec: 23425.623471\n",
            "Epoch Step: 400 Loss: 49.828968 Tokens per Sec: 23158.654155\n",
            "Epoch Step: 500 Loss: 30.854010 Tokens per Sec: 23359.636706\n",
            "Epoch Step: 600 Loss: 38.795143 Tokens per Sec: 23366.553801\n",
            "Epoch Step: 700 Loss: 60.651134 Tokens per Sec: 23425.040139\n",
            "Epoch Step: 800 Loss: 39.381969 Tokens per Sec: 23102.885816\n",
            "Epoch Step: 900 Loss: 35.081059 Tokens per Sec: 23137.879077\n",
            "Epoch Step: 1000 Loss: 38.140266 Tokens per Sec: 23223.018090\n",
            "Epoch Step: 1100 Loss: 34.178162 Tokens per Sec: 23098.314354\n",
            "Epoch Step: 1200 Loss: 25.684589 Tokens per Sec: 23303.634048\n",
            "Epoch Step: 1300 Loss: 13.210073 Tokens per Sec: 23174.335177\n",
            "Epoch Step: 1400 Loss: 31.474247 Tokens per Sec: 23583.793312\n",
            "Epoch Step: 1500 Loss: 14.461681 Tokens per Sec: 23118.730809\n",
            "Epoch Step: 1600 Loss: 14.938192 Tokens per Sec: 23052.773165\n",
            "Epoch Step: 1700 Loss: 9.264913 Tokens per Sec: 23214.468537\n",
            "Epoch Step: 1800 Loss: 18.190861 Tokens per Sec: 23188.050646\n",
            "Epoch Step: 1900 Loss: 9.928220 Tokens per Sec: 23055.456014\n",
            "Epoch Step: 2000 Loss: 22.479858 Tokens per Sec: 22997.150074\n",
            "Epoch Step: 2100 Loss: 7.102470 Tokens per Sec: 23048.434248\n",
            "Epoch Step: 2200 Loss: 58.663059 Tokens per Sec: 23176.758051\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was able to have a <unk> from the morning of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , gray radio the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty much , because it was the news most <unk> .\n",
            "\n",
            "Validation perplexity: 12.538198\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 33.332603 Tokens per Sec: 22270.381372\n",
            "Epoch Step: 200 Loss: 13.903094 Tokens per Sec: 23030.466533\n",
            "Epoch Step: 300 Loss: 16.693563 Tokens per Sec: 23085.670864\n",
            "Epoch Step: 400 Loss: 23.674568 Tokens per Sec: 23275.185329\n",
            "Epoch Step: 500 Loss: 31.569336 Tokens per Sec: 23189.795620\n",
            "Epoch Step: 600 Loss: 42.415257 Tokens per Sec: 23125.536775\n",
            "Epoch Step: 700 Loss: 17.580065 Tokens per Sec: 22917.918576\n",
            "Epoch Step: 800 Loss: 31.846941 Tokens per Sec: 23054.102042\n",
            "Epoch Step: 900 Loss: 51.368580 Tokens per Sec: 22957.014789\n",
            "Epoch Step: 1000 Loss: 12.663275 Tokens per Sec: 23182.655474\n",
            "Epoch Step: 1100 Loss: 44.377045 Tokens per Sec: 22821.308054\n",
            "Epoch Step: 1200 Loss: 2.598039 Tokens per Sec: 23098.531296\n",
            "Epoch Step: 1300 Loss: 25.873734 Tokens per Sec: 23286.779242\n",
            "Epoch Step: 1400 Loss: 21.733597 Tokens per Sec: 23358.143592\n",
            "Epoch Step: 1500 Loss: 31.891926 Tokens per Sec: 23455.519752\n",
            "Epoch Step: 1600 Loss: 30.537661 Tokens per Sec: 23225.313209\n",
            "Epoch Step: 1700 Loss: 46.468781 Tokens per Sec: 22948.991859\n",
            "Epoch Step: 1800 Loss: 22.297726 Tokens per Sec: 23078.921843\n",
            "Epoch Step: 1900 Loss: 43.165852 Tokens per Sec: 23216.109005\n",
            "Epoch Step: 2000 Loss: 31.497438 Tokens per Sec: 23316.116412\n",
            "Epoch Step: 2100 Loss: 20.386946 Tokens per Sec: 23193.932881\n",
            "Epoch Step: 2200 Loss: 52.971191 Tokens per Sec: 22982.888578\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was born in the morning of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father stopped on his little , gray radio the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , which was very unusual , which was the most exciting thing that <unk> it .\n",
            "\n",
            "Validation perplexity: 11.797158\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 29.097702 Tokens per Sec: 21779.490699\n",
            "Epoch Step: 200 Loss: 18.449196 Tokens per Sec: 23041.718555\n",
            "Epoch Step: 300 Loss: 34.454556 Tokens per Sec: 22571.962878\n",
            "Epoch Step: 400 Loss: 19.459219 Tokens per Sec: 22706.770228\n",
            "Epoch Step: 500 Loss: 28.194445 Tokens per Sec: 22862.178811\n",
            "Epoch Step: 600 Loss: 21.313402 Tokens per Sec: 23252.950144\n",
            "Epoch Step: 700 Loss: 7.172859 Tokens per Sec: 23135.031478\n",
            "Epoch Step: 800 Loss: 45.712105 Tokens per Sec: 22808.431655\n",
            "Epoch Step: 900 Loss: 28.029730 Tokens per Sec: 23102.519123\n",
            "Epoch Step: 1000 Loss: 58.010712 Tokens per Sec: 23226.504954\n",
            "Epoch Step: 1100 Loss: 13.585089 Tokens per Sec: 22815.263624\n",
            "Epoch Step: 1200 Loss: 26.092756 Tokens per Sec: 22948.012935\n",
            "Epoch Step: 1300 Loss: 43.681973 Tokens per Sec: 23151.289359\n",
            "Epoch Step: 1400 Loss: 14.902041 Tokens per Sec: 23091.670361\n",
            "Epoch Step: 1500 Loss: 40.293407 Tokens per Sec: 23105.653471\n",
            "Epoch Step: 1600 Loss: 27.959328 Tokens per Sec: 23101.334554\n",
            "Epoch Step: 1700 Loss: 49.184448 Tokens per Sec: 23316.246217\n",
            "Epoch Step: 1800 Loss: 21.474972 Tokens per Sec: 23260.313451\n",
            "Epoch Step: 1900 Loss: 37.294281 Tokens per Sec: 23285.456055\n",
            "Epoch Step: 2000 Loss: 46.445606 Tokens per Sec: 23064.591262\n",
            "Epoch Step: 2100 Loss: 7.234984 Tokens per Sec: 23316.912434\n",
            "Epoch Step: 2200 Loss: 52.109707 Tokens per Sec: 23247.119769\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i became a <unk> of the morning in the morning .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father listened to his little , gray radio the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was quite unusual , because it was the news most <unk> .\n",
            "\n",
            "Validation perplexity: 11.776445\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 47.563881 Tokens per Sec: 21894.071526\n",
            "Epoch Step: 200 Loss: 46.629604 Tokens per Sec: 23044.210015\n",
            "Epoch Step: 300 Loss: 21.494667 Tokens per Sec: 22851.242361\n",
            "Epoch Step: 400 Loss: 2.202330 Tokens per Sec: 22827.408563\n",
            "Epoch Step: 500 Loss: 38.787212 Tokens per Sec: 23008.887012\n",
            "Epoch Step: 600 Loss: 47.283287 Tokens per Sec: 23218.008039\n",
            "Epoch Step: 700 Loss: 8.313066 Tokens per Sec: 23181.704034\n",
            "Epoch Step: 800 Loss: 23.851810 Tokens per Sec: 22834.639446\n",
            "Epoch Step: 900 Loss: 27.954567 Tokens per Sec: 23045.698429\n",
            "Epoch Step: 1000 Loss: 40.957291 Tokens per Sec: 23101.137641\n",
            "Epoch Step: 1100 Loss: 33.955673 Tokens per Sec: 23275.821646\n",
            "Epoch Step: 1200 Loss: 50.948311 Tokens per Sec: 23222.647515\n",
            "Epoch Step: 1300 Loss: 33.487164 Tokens per Sec: 23162.406149\n",
            "Epoch Step: 1400 Loss: 8.624795 Tokens per Sec: 23616.824047\n",
            "Epoch Step: 1500 Loss: 28.719780 Tokens per Sec: 23153.811261\n",
            "Epoch Step: 1600 Loss: 40.524986 Tokens per Sec: 22877.877147\n",
            "Epoch Step: 1700 Loss: 7.064911 Tokens per Sec: 23159.349439\n",
            "Epoch Step: 1800 Loss: 25.603727 Tokens per Sec: 22991.828021\n",
            "Epoch Step: 1900 Loss: 48.160164 Tokens per Sec: 23093.379898\n",
            "Epoch Step: 2000 Loss: 48.298138 Tokens per Sec: 23073.405245\n",
            "Epoch Step: 2100 Loss: 37.559811 Tokens per Sec: 23369.058355\n",
            "Epoch Step: 2200 Loss: 48.905521 Tokens per Sec: 23033.934085\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was born in a morning by the morning of joseph .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard on his little , gray radio the bbc 's <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , which was very unusual , which was really the news of the news .\n",
            "\n",
            "Validation perplexity: 11.752918\n",
            "Epoch 9\n",
            "Epoch Step: 100 Loss: 8.097419 Tokens per Sec: 21940.512002\n",
            "Epoch Step: 200 Loss: 39.797245 Tokens per Sec: 23170.243160\n",
            "Epoch Step: 300 Loss: 46.149994 Tokens per Sec: 22936.672927\n",
            "Epoch Step: 400 Loss: 11.167440 Tokens per Sec: 23087.728803\n",
            "Epoch Step: 500 Loss: 12.287391 Tokens per Sec: 23320.388780\n",
            "Epoch Step: 600 Loss: 33.645935 Tokens per Sec: 23248.838490\n",
            "Epoch Step: 700 Loss: 43.291359 Tokens per Sec: 23122.272970\n",
            "Epoch Step: 800 Loss: 30.050091 Tokens per Sec: 23163.684050\n",
            "Epoch Step: 900 Loss: 20.676754 Tokens per Sec: 22901.159585\n",
            "Epoch Step: 1000 Loss: 43.250095 Tokens per Sec: 23342.380037\n",
            "Epoch Step: 1100 Loss: 25.953596 Tokens per Sec: 23048.831129\n",
            "Epoch Step: 1200 Loss: 34.664749 Tokens per Sec: 23307.170122\n",
            "Epoch Step: 1300 Loss: 40.630909 Tokens per Sec: 22977.092339\n",
            "Epoch Step: 1400 Loss: 16.259699 Tokens per Sec: 23215.276785\n",
            "Epoch Step: 1500 Loss: 12.275828 Tokens per Sec: 23183.220364\n",
            "Epoch Step: 1600 Loss: 9.027254 Tokens per Sec: 23179.728808\n",
            "Epoch Step: 1700 Loss: 39.113003 Tokens per Sec: 23307.393755\n",
            "Epoch Step: 1800 Loss: 32.724323 Tokens per Sec: 23184.371420\n",
            "Epoch Step: 1900 Loss: 42.146107 Tokens per Sec: 23047.363761\n",
            "Epoch Step: 2000 Loss: 12.910942 Tokens per Sec: 23187.079168\n",
            "Epoch Step: 2100 Loss: 48.646412 Tokens per Sec: 23151.029498\n",
            "Epoch Step: 2200 Loss: 32.662029 Tokens per Sec: 23064.422163\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was born in the morning of the morning of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father heard about his little , gray radio the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he looked very happy , which was very unusual , which was the news for <unk> .\n",
            "\n",
            "Validation perplexity: 11.983217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swBj18hfNmO3",
        "outputId": "bbf7bae5-a922-4478-d6df-3faf737f160c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PtLnVP2NqRn"
      },
      "source": [
        "import sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQEwv1v1Nr_P",
        "outputId": "8f0a725b-2fb2-4fb3-85be-356bd967da39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# this should result in a perfect BLEU of 100%\n",
        "hypotheses = [\"this is a test\"]\n",
        "references = [\"this is a test\"]\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.00000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOpybeY9Ntbv",
        "outputId": "719a87fd-8296-4924-832b-cd0f77379bdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(valid_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqfQ2dJSNvPo",
        "outputId": "4c5fb498-6c9a-4d2f-a4a2-c732ebf15597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "references = [\" \".join(example.trg) for example in valid_data]\n",
        "print(len(references))\n",
        "print(references[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "when i was 11 , i remember waking up one morning to the sound of joy in my house .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNetzVhdNvow",
        "outputId": "7f4673b4-7436-4bd1-c3d0-50560616af17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "references[-2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i 'm always the one taking the picture .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdT_iqFdNvv2",
        "outputId": "94802e1f-d1df-4805-fbea-caf930d1599b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hypotheses = []\n",
        "alphas = []  # save the last attention scores\n",
        "for batch in valid_iter:\n",
        "  batch = rebatch(PAD_INDEX, batch)\n",
        "  pred, attention = greedy_decode(\n",
        "    model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
        "    sos_index=TRG.vocab.stoi[SOS_TOKEN],\n",
        "    eos_index=TRG.vocab.stoi[EOS_TOKEN])\n",
        "  hypotheses.append(pred)\n",
        "  alphas.append(attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm3dxUmQNvzq",
        "outputId": "b6d5a09e-71ab-42b1-b1eb-c89f3ba5bf36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we will still need to convert the indices to actual words!\n",
        "hypotheses[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  70,   11,   24, 1460,  103,  217,    5,   11,   24,  562,   16,\n",
              "          6,  690,   10,    6,  690,   10,    6,    0,    4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRZ43o4GNv2m",
        "outputId": "2afe46d0-f17c-4125-f34c-f8fb09d64d35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
        "hypotheses[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['when',\n",
              " 'i',\n",
              " 'was',\n",
              " '11',\n",
              " 'years',\n",
              " 'old',\n",
              " ',',\n",
              " 'i',\n",
              " 'was',\n",
              " 'born',\n",
              " 'in',\n",
              " 'the',\n",
              " 'morning',\n",
              " 'of',\n",
              " 'the',\n",
              " 'morning',\n",
              " 'of',\n",
              " 'the',\n",
              " '<unk>',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0-_gqJiNv5O",
        "outputId": "659b06e8-a680-4904-a9ab-2c40f4843b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# finally, the SacreBLEU raw scorer requires string input, so we convert the lists to strings\n",
        "hypotheses = [\" \".join(x) for x in hypotheses]\n",
        "print(len(hypotheses))\n",
        "print(hypotheses[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "690\n",
            "when i was 11 years old , i was born in the morning of the morning of the <unk> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q32qILBFNv8I",
        "outputId": "f8cf69b6-c93a-4943-f673-44e9a3ab2767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# now we can compute the BLEU score!\n",
        "bleu = sacrebleu.raw_corpus_bleu(hypotheses, [references], .01).score\n",
        "print(bleu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24.00860444696703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TPEHjcWNv-p"
      },
      "source": [
        "def plot_heatmap(src, trg, scores):\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    heatmap = ax.pcolor(scores, cmap='viridis')\n",
        "\n",
        "    ax.set_xticklabels(trg, minor=False, rotation='vertical')\n",
        "    ax.set_yticklabels(src, minor=False)\n",
        "\n",
        "    # put the major ticks at the middle of each cell\n",
        "    # and the x-ticks on top\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
        "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    plt.colorbar(heatmap)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X5VG2niNwBX",
        "outputId": "289840b1-1656-41dd-f276-e855a263e35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# This plots a chosen sentence, for which we saved the attention scores above.\n",
        "idx = 5\n",
        "src = valid_data[idx].src + [\"</s>\"]\n",
        "trg = valid_data[idx].trg + [\"</s>\"]\n",
        "pred = hypotheses[idx].split() + [\"</s>\"]\n",
        "pred_att = alphas[idx][0].T[:, :len(pred)]\n",
        "print(\"src\", src)\n",
        "print(\"ref\", trg)\n",
        "print(\"pred\", pred)\n",
        "plot_heatmap(src, pred, pred_att)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src ['\"', 'jetzt', 'kannst', 'du', 'auf', 'eine', 'richtige', 'schule', 'gehen', ',', '\"', 'sagte', 'er', '.', '</s>']\n",
            "ref ['\"', 'you', 'can', 'go', 'to', 'a', 'real', 'school', 'now', ',', '\"', 'he', 'said', '.', '</s>']\n",
            "pred ['\"', 'now', 'you', 'can', 'go', 'to', 'a', 'real', 'school', ',', '\"', 'he', 'said', '.', '</s>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOrJW9vvNwEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}